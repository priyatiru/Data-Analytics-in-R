n2<-ifelse(V(G)[some_x]$action<-"A",num+1,num+1)
}
#return(num)
payoff_A=a*n2
payoff_B=b*n2
V(G)$payoff<-ifelse(payoff_A>=payoff_B,list2<-c(V(G)$action=="A"),list2-c(V(G)$action=="B"))
}
V(G)[V(G)$name %in% list2]$action = list2
V(G)$color <- ifelse(V(G)$action == list2, "green", "red")
plot(G)
#Payoff of A=a=4
#Payoff of B=b=3
a=4
b=3
list2<-list()
for (i in 1:vcount(G)) {
num=0
x<-neighbors(G,i)
for (j in x) {
V(G)$num2<-ifelse(V(G)[x]$action<-"A",num+1,num+1)
}
#return(num)
payoff_A=a*(V(G)$num2)
payoff_B=b*(V(G)$num2)
V(G)$payoff<-ifelse(payoff_A>=payoff_B,list2<-c(V(G)$action=="A"),list2-c(V(G)$action=="B"))
}
V(G)[V(G)$name %in% list2]$action = list2
V(G)$color <- ifelse(V(G)$action == list2, "green", "red")
plot(G)
#Payoff of A=a=4
#Payoff of B=b=3
a=4
b=3
list2<-list()
for (i in 1:vcount(G)) {
num=0
x<-neighbors(G,i)
for (j in x) {
V(G)$num2<-ifelse(V(G)[x]$action<-"A",num+1,num+1)
}
payoff_A=a*(V(G)$num2)
payoff_B=b*(V(G)$num2)
V(G)$payoff<-ifelse(payoff_A>=payoff_B,list2<-c(V(G)$action=="A"),list2-c(V(G)$action=="B"))
}
V(G)[V(G)$name %in% list2]$action = list2
V(G)$color <- ifelse(V(G)$action == list2, "green", "red")
V(G)$color <- ifelse(V(G)[x]$action == list2, "green", "red")
V(G)$color <- ifelse(V(G)$action == list2, "green", "red")
V(G)$color <- ifelse(V(G)$action == list2, "green", "red")
plot(G)
V(G)$payoff<-ifelse(payoff_A>=payoff_B,list2<-c(V(G)$action=="A"),list2<-c(V(G)$action=="B"))
for (i in 1:vcount(G)) {
num=0
x<-neighbors(G,i)
for (j in x) {
V(G)$num2<-ifelse(V(G)[x]$action<-"A",num+1,num+1)
}
payoff_A=a*(V(G)$num2)
payoff_B=b*(V(G)$num2)
V(G)$payoff<-ifelse(payoff_A>=payoff_B,list2<-c(V(G)$action=="A"),list2<-c(V(G)$action=="B"))
}
V(G)[V(G)$name %in% list2]$action = list2
V(G)$color <- ifelse(V(G)$action == list2, "green", "red")
plot(G)
for (i in 1:vcount(G)) {
num=0
x<-neighbors(G,i)
for (j in x) {
V(G)$num2<-ifelse(V(G)[x]$action<-"A",num+1,num+1)
}
payoff_A=a*(V(G)$num2)
payoff_B=b*(V(G)$num2)
V(G)$payoff<-ifelse(payoff_A>=payoff_B,list2<-c(V(G)$action=="A"),list2<-c(V(G)$action=="B"))
}
V(G)[V(G)$name %in% list2]$action = list2
V(G)$color <- ifelse(V(G)$action == list2, "green", "red")
plot(G)
x=rnorm(10)
y=rnorm(10)
t.test(x,y)
install.packages("dplyr")
source('C:/Users/tirufamily/Desktop/hypothesis.R', echo=TRUE)
#Given data is of the two populations being compared are “men who have not taken caffeine”
#and “men who have taken caffeine”.
placebo<-c(105,119,100,97,96,101,94,95,98)
caffeine<-c(96,99,94,89,96,93,88,105,88)
#Creating dataframe for both the population
pop_data<-data.frame(Types=c(rep("placebo",9),rep("caffeine",9)),
num_tubes = c(placebo,  caffeine))
pop_data
group_by(pop_data, Types)
group_by(pop_data, Types) %>% summarise(sample_size = n(),
sample_mean = mean(num_tubes, na.rm = TRUE),
sample_sd = sd(num_tubes, na.rm = TRUE))
t.test(placebo,caffeine)
biology <- c(3, 7, 11, 0, 7, 0, 4, 5, 6, 2, 4, 7, 2, 9)
english <- c(8, 5, 4, 10, 4, 5, 7, 2, 6, 1, 2, 7, 0, 6, 4, 12, 5, 2)
# Creating data frame
test_results <- data.frame(
score = c(biology, english),
department = c(rep("biology", 14),
rep("english",18)
)
)
delta_0<-0
#an assumption
sigma_sq_1 <- 3
sigma_sq_2 <- 2
n1<-14
n2<-18
#calculating the z-statistic
z_stat <- (mean(biology) - mean(english) - delta_0) /
sqrt(sigma_sq_1 / n1 + sigma_sq_2 / n2)
z_stat
# Given data
y1=c(45, 87, 123, 120, 70)
y2=c(51, 71, 42, 37, 51, 78, 51, 49, 56, 47, 58)
var.test(y1,y2)
######### 2. PAIRED T-TEST  ########
#Given data
x1=c(7.3, 6.8, 7.3, 4.8, 5.6, 6.2, 5.6, 4.5, 6.3, 6.7, 5.4, 6.8, 6.5, 7.7, 8.3)
x2=c(9.1, 8.6, 7.1, 9.6, 9.7, 9.8, 9.1, 5.2, 9.4, 9.3, 4.9, 10.1, 6.3, 10.2, 10.9)
t.test(x1,x1,paired = TRUE)
######### CHI-SQUARED TEST FOR NOMINAL (CATEGIORICAL) DATA ########
A<-c(38,33,42,26,11)
B<-c(72,57,38,44,29)
M <- as.table(rbind(A,B))
M
(Xsq <- chisq.test(M))
Xsq$observed
Xsq$expected
######### 1. ONE SAMPLE T-TEST (INDEPENDENT T-TEST) ########
x<-c(3,7,11,0,7,0,4,5,6,2)
t_stat<-(mean(x)-3/(sd(x)/sqrt(length(x))))
t_stat
t.test(x,mu=3)
######### 4. ONE SAMPLE Z-TEST  ########
x<-c(3,7,11,0,7,0,4,5,6,2)
n<length(x)
n<-length(x)
z_stat<-(mean(x)-3/(2/sqrt(n)))
z_stat
######### 4. ONE SAMPLE Z-TEST  ########
#Given data : suppose that a student is interesting in estimating
#how many memes their professors know and love.
#So they go to class, and every time a professor uses a new meme,
#they write it down. After a year of classes,
#the student has recorded the following meme counts,
#where each count corresponds to a single class they took:
x<-c(3,7,11,0,7,0,4,5,6,2)
n<-length(x)
z_stat<-(mean(x)-3/(2/sqrt(n)))    #standard deviation is given = 2
z_stat
######### 5. TWO SAMPLE Z-TEST ########
#Given data :  suppose that a student wants to figure out if biology professors or
#English professors know more memes. The student writes a meme quiz
#and springs it on 14 unsuspecting biology professors
#and 18 unsuspecting English professors during office hours.
biology <- c(3, 7, 11, 0, 7, 0, 4, 5, 6, 2, 4, 7, 2, 9)
english <- c(8, 5, 4, 10, 4, 5, 7, 2, 6, 1, 2, 7, 0, 6, 4, 12, 5, 2)
# Creating data frame
test_results <- data.frame(
score = c(biology, english),
department = c(rep("biology", 14),
rep("english",18)
)
)
delta_0<-0
#an assumption
sigma_sq_1 <- 3
sigma_sq_2 <- 2
n1<-14
n2<-18
#calculating the z-statistic
z_stat <- (mean(biology) - mean(english) - delta_0) /
sqrt(sigma_sq_1 / n1 + sigma_sq_2 / n2)
z_stat
######### 1. ONE SAMPLE T-TEST (INDEPENDENT T-TEST) ########
#Given data : suppose that a student is interesting in estimating
#how many memes their professors know and love.
#So they go to class, and every time a professor uses a new meme,
#they write it down. After a year of classes,
#the student has recorded the following meme counts,
#where each count corresponds to a single class they took:
x<-c(3,7,11,0,7,0,4,5,6,2)
t.test(x,mu=3)
######### 2. TWO SAMPLE T-TEST (INDEPENDENT T-TEST) ########
#Given data is of the two populations being compared are
#“men who have not taken caffeine”
#and “men who have taken caffeine”.
placebo<-c(105,119,100,97,96,101,94,95,98)
caffeine<-c(96,99,94,89,96,93,88,105,88)
#Creating dataframe for both the population
pop_data<-data.frame(Types=c(rep("placebo",9),rep("caffeine",9)),
num_tubes = c(placebo,  caffeine))
pop_data
group_by(pop_data, Types) %>%
summarise(sample_size = n(),
sample_mean = mean(num_tubes, na.rm = TRUE),
sample_sd = sd(num_tubes, na.rm = TRUE))
t.test(placebo,caffeine)
######### 3. PAIRED T-TEST  ########
#Given data
x1=c(7.3, 6.8, 7.3, 4.8, 5.6, 6.2, 5.6, 4.5,
6.3, 6.7, 5.4, 6.8, 6.5, 7.7, 8.3)
x2=c(9.1, 8.6, 7.1, 9.6, 9.7, 9.8, 9.1, 5.2,
9.4, 9.3, 4.9, 10.1, 6.3, 10.2, 10.9)
t.test(x1,x1,paired = TRUE)
################# 6. F-Test FOR COMPARING TWO VARIENCES ########
# Given data
y1=c(45, 87, 123, 120, 70)
y2=c(51, 71, 42, 37, 51, 78, 51, 49, 56, 47, 58)
var.test(y1,y2)
######### 7. CHI-SQUARED TEST FOR NOMINAL (CATEGIORICAL) DATA ########
A<-c(38,33,42,26,11)
B<-c(72,57,38,44,29)
M <- as.table(rbind(A,B))
M
(Xsq <- chisq.test(M))
Xsq$observed
Xsq$expected
source('C:/Users/tirufamily/Desktop/hypothesis.R')
source('~/.active-rstudio-document', echo=TRUE)
library("dplyr")
pop_mean<-100
sample_mean<-140
n<-30
s_d<-15
t_stat<-(sample_mean-pop_mean/(s_d/(sqrt(n))))
t_stat
library(foreign)
data<-read.dta("C:\Users\tirufamily\Desktop\Linear model\MentalHealth.dta")
data<-read.dta("C:/Users/tirufamily/Desktop/Linear model/MentalHealth.dta")
data
library(ggplot)
install.packages("ggplot")
pairs(~mentalImpair+lifeEvents+ses, data)
#(b.)
slr1 <- lm(mentalImpair ~ lifeEvents, data)
slr1
summary(slr1)
t.test(mentalImpair,lifeEvents)
t.test(slr1)
t_val
t_val<-2.471^2
t_val
#(c.)
slr_2 <- lm(mentalImpair ~ 1, data)
rss <- function(model) sum(residuals(model)^2)
p <- rss(slr_1)/rss(slr_2)   #pearsons correlation coefficient
#(c.)
slr2 <- lm(mentalImpair ~ 1, data)
rss <- function(model) sum(residuals(model)^2)
p <- rss(slr1)/rss(slr2)   #pearsons correlation coefficient
p
c(p, sqrt(p))
rss
library(dpylr)
library(dplyr)
#(d.)
data <- mutate(data, lifeCsq = (lifeEvents - mean(lifeEvents))^2)
data
slr3 <- lm(mentalImpair ~ lifeEvents + lifeCsq, data)
slr3
summary(slr3)
#(e.)
slr4 <- lm(mentalImpair ~ ses, data)
cor(data$mentalImpair, data$ses)
std
#std
slr5 <- lm(std(mentalImpair) ~ std(ses), data)
std <- function(x) (x - mean(x))/sqrt(var(x))
#std
slr5 <- lm(std(mentalImpair) ~ std(ses), data)
summary(slr5)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
install.packages(tidytext)
library(tidytext)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
install.packages(tidytext)
install.packages("tidytext")
source('~/.active-rstudio-document', echo=TRUE)
data_df %>%
unnest_tokens(word,data)
unnest_tokens(word,text
data_df %>%
unnest_tokens(word,text)
data_df %>%
unnest_tokens(data_df,token="words")
data_df
#1.2 Tokenization
library(tidytext)
data_df %>%
unnest_tokens(data_df,token="words")
data_df %>%
unnest_tokens(token="words",data)
library(tidyverse)
install.packages("tidyverse")
setwd("C:/Users/tirufamily/Desktop/Text_analytics")
library(dplyr)
library(plyr)
library(NLP)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(tidytext)
library(tidyverse)
tweets.df <- read.csv("C:/Users/tirufamily/Desktop/Text_analytics/demonetization-tweets.csv")
str(tweets.df)
tweets <- as.character(tweets.df$text)
head(tweets)
tweets <- gsub('[[:cntrl:]]', '', tweets)
# remove retweets
tweets <- gsub('(RT|via)((?:\\b\\W*@\\W+)+)', '', tweets)
head(tweets)
# remove at people
tweets <- gsub('@\\w+', '', tweets)
# remove punctuations
tweets<- gsub('[[:punct:]]', '', tweets)
# remove numbers
tweets <- gsub('[[:digit:]]', '', tweets)
# remove html links
tweets <- gsub('http[s]?\\w+', '', tweets)
# remove extra spaces
tweets <- gsub('[ \t]{2,}', '', tweets)
tweets<- gsub('^\\s+|\\s+$', '', tweets)
# removing NA's
tweets <- tweets[!is.na(tweets)]
# convert to lower case:
tweets <- tolower(tweets)
head(tweets)
#Store the text in the corpus
t_corpus = VCorpus(VectorSource(tweets))
#remove stopwords
t_corpus = tm_map(t_corpus,removeWords,c(stopwords("en"),"amp","demonetiztion","obqrhlnsl","uodwxdpmmg"))
#remove whitespace
t_corpus = tm_map(t_corpus,stripWhitespace)
#create a Term Document Matrix
t_tdm = TermDocumentMatrix(t_corpus)
#convert into matrix
t_matrix = as.matrix(t_tdm)
#calculate word frequency
word_freq = rowSums(t_matrix)
word_freq = sort(word_freq,decreasing = TRUE)
#plotting the frequency of words
barplot(word_freq[1:10],col = "black",las = 2,main = "Frequency plot of words")
word_freq2 = data.frame(term = names(word_freq),num = word_freq)
#creating word cloud
#creating word cloud
wordcloud(word_freq2$term,word_freq2$num,min.freq = 150,max.words = 100,random.order = 'F',rot.per = 0.1,colors = brewer.pal(8,"Dark2"),scale = c(3,0.3),random.color = T)
library(ggplot2)
mysentiment<-get_nrc_sentiment((tweets))
library(sentiment)
mysentiment<-get_nrc_sentiment(tweets)
library(sentiment)
install.packages("sentiment")
negative.matches <- match(tweets), negative.words)
negative.matches <- match(tweets, negative.words)
negative.matches <- match(tweets, negative_words)
negative.matches <- match(tweets, "~/negative_words")
positive.matches <- match(words, "~/positive.words")
head(negative.matches)
head(positive.matches)
negative.matches <- match(tweets, "~/negative_words")
positive.matches <- match(tweets, "~/positive.words")
positive.matches <- !is.na(positive.matches)
negative.matches <- !is.na(negative.matches)
score <- sum(positive.matches) - sum(negative.matches)
score
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores <- laply(sentences, function(sentence, positive.words, negative.words)
{
word.list <- str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words <- unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
negative.matches <- match(words, negative.words)
positive.matches <- match(words, positive.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
positive.matches <- !is.na(positive.matches)
negative.matches <- !is.na(negative.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(positive.matches) - sum(negative.matches)
return(score)
}, positive.words, negative.words, .progress=.progress )
sentiment.score <- function(sentences, positive.words, negative.words)
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores <- laply(sentences, function(sentence, positive.words, negative.words)
{
word.list <- str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words <- unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
negative.matches <- match(words, negative.words)
positive.matches <- match(words, positive.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
positive.matches <- !is.na(positive.matches)
negative.matches <- !is.na(negative.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(positive.matches) - sum(negative.matches)
return(score)
}, positive.words, negative.words, .progress=.progress )
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
#bag of words
positive <- scan("positive.words.txt", what= "character", comment.char= ";")
negative <- scan("negative.words.txt", what= "character", comment.char= ";")
tweets.analysis <- sentiment.score(tweets, positive, negative, .progress="none")
sentiment.score <- function(sentences, positive.words, negative.words,.progress="none")
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores <- laply(sentences, function(sentence, positive.words, negative.words)
{
word.list <- str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words <- unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
negative.matches <- match(words, negative.words)
positive.matches <- match(words, positive.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
positive.matches <- !is.na(positive.matches)
negative.matches <- !is.na(negative.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(positive.matches) - sum(negative.matches)
return(score)
}, positive.words, negative.words, .progress=.progress )
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
#bag of words
positive <- scan("positive.words.txt", what= "character", comment.char= ";")
negative <- scan("negative.words.txt", what= "character", comment.char= ";")
tweets.analysis <- sentiment.score(tweets, positive, negative, .progress="none")
str(tweets.analysis)
tweets.analysis$sentiment[tweets.analysis$score == 0] <- "Neutral"
tweets.analysis$sentiment[tweets.analysis$score < 0] <- "Negative"
tweets.analysis$sentiment[tweets.analysis$score > 0] <- "Positive"
tweets.analysis$sentiment <- factor(tweets.analysis$sentiment)
table(tweets.analysis$score)
mean(tweets.analysis$score) # slighlty positive
median(tweets.analysis$score)
summary(tweets.analysis$sentiment) # more positive tweets than negative
#Plotting of the tweets
ggplot(data = tweets.analysis, aes(x = score, fill = sentiment)) +
geom_bar() +
labs(title = "Sentiment Score Bar Plot", x = "Sentiment Score", y = "Tweet Count") +
scale_x_continuous(breaks = seq(-6,6,1)) +
scale_y_continuous(breaks = seq(0,4000,500)) +
scale_fill_manual(guide = guide_legend("Sentiment"), values = c("#DD0426","#246EB9","#04B430"))
#creating word cloud
wordcloud(word_freq2$term,word_freq2$num,min.freq = 150,max.words = 100,random.order = 'F',rot.per = 0.1,colors = brewer.pal(8,"Dark2"),scale = c(3,0.5),random.color = T)
#hypothesis testing
sd(score)
size(score)
length(score)
mu0 = 0
z = (mean - mu0)/(sd(score)/sqrt(length(score)))
#hypothesis testing
sd(tweets.analysis$score)
#hypothesis testing
sd<-sd(tweets.analysis$score)
sd
size(tweets.analysis$score)
size<-size(tweets.analysis$score)
#hypothesis testing
sd<-sd(tweets.analysis$score)
sd
len<-length(tweets.analysis$score)
len
mu0 = 0
z = (mean - mu0)/(sd/sqrt(len))
z
alpha = 0.05
z.alpha = qnorm(1-alpha)
z.alpha
mu0<-0
z = (mean - mu0)/(sd/sqrt(len))
mu0<-0.0
z = (mean - mu0)/(sd/sqrt(len))
z = (mean - 0)/(sd/sqrt(len))
z = (mean)/(sd/sqrt(len))
z = (mean)/(sd/sqrt(len))
#statistical inferences
mean(tweets.analysis$score) # slighlty positive
#hypothesis testing
sd<-sd(tweets.analysis$score)
sd
len<-length(tweets.analysis$score)
len
alpha = 0.05
z.alpha = qnorm(1-alpha)
z.alpha
#plotting the frequency of words
barplot(word_freq[1:10],col = "black",las = 2,main = "Frequency plot of words")
